{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3245,
   "id": "bb9a3e40-f49d-41ff-aa0c-780b4d6652c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3247,
   "id": "7f825c92-2bcc-4476-80fa-bb9130679afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 4\n",
    "num_heads=2\n",
    "b = 2\n",
    "s = 8\n",
    "x = torch.randn(b,s,emb_dim) #b,s,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3394,
   "id": "3e0895fa-c3d4-454d-849b-8ac24a7eb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCrossAttention(nn.Module):\n",
    "    def __init__(self,emb_dim=emb_dim,num_heads=num_heads):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.Q = nn.Linear(emb_dim,emb_dim,bias=False,dtype=float)\n",
    "        self.K = nn.Linear(emb_dim,emb_dim,bias=False,dtype=float)\n",
    "        self.V = nn.Linear(emb_dim,emb_dim,bias=False,dtype=float)\n",
    "        self.proj = nn.Linear(emb_dim,emb_dim,dtype=float)\n",
    "    def forward(self,x,x2=None,x3=None,mask=None):\n",
    "        x = x.to(float)\n",
    "        # print(\"forwards\")\n",
    "        b,s,d = x.shape\n",
    "        q = self.Q(x) #b,s,d d,d = b,s,d\n",
    "        if x2!=None:\n",
    "            x2 = x2.to(float)\n",
    "            k = self.K(x2)\n",
    "        else:\n",
    "            k = self.K(x)\n",
    "        if x3!=None:\n",
    "            x3 = x3.to(float)\n",
    "            v = self.V(x3)\n",
    "        else:\n",
    "            v = self.V(x)\n",
    "        # q,k,v = b,s,d ()\n",
    "        q = q.view(b,self.num_heads,s,d//self.num_heads)\n",
    "        k = k.view(b,self.num_heads,s,d//self.num_heads)\n",
    "        v = v.view(b,self.num_heads,s,d//self.num_heads)\n",
    "        # q,k,v = b,h,s,d/h -> q.kT (b,h,s,d/h * b,h,d/h,s)\n",
    "        attn_scores = (q@k.transpose(-1,-2)/(self.emb_dim//self.num_heads)**(0.5))\n",
    "        # print(attn_scores.shape)\n",
    "        if mask is not None:\n",
    "            attn_scores = torch.masked_fill(attn_scores,mask,-torch.inf)\n",
    "        # print(attn_scores.shape)\n",
    "            \n",
    "        attn_weights = torch.softmax(attn_scores,dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights,p=0.1,training=self.training)\n",
    "        # attn_weights = b,h,s,s v = b,h,s,d/h => b,h,s,d/h => b,s,d \n",
    "        attn = attn_weights@v\n",
    "        # print(attn.shape)\n",
    "        attn = attn.view(b,s,d)\n",
    "        output = self.proj(attn)\n",
    "        return output.to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3396,
   "id": "566f929a-45d2-4ba7-a3dc-13bbaa782323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,emb_dim,exp_factor=4):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(emb_dim , exp_factor*emb_dim).to(float)\n",
    "        self.r  = nn.ReLU()\n",
    "        self.l2 = nn.Linear(exp_factor*emb_dim ,emb_dim).to(float)\n",
    "    def forward(self,x):\n",
    "        x = x.to(float)\n",
    "        return self.l2(self.r(self.l1(x))).to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3398,
   "id": "551d72ef-d2f7-481b-b3f9-9d0525dc45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,emb_dim,num_heads,exp_factor=4):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttentionWithCrossAttention(emb_dim,num_heads).to(float)\n",
    "        self.mlp = MLP(emb_dim,exp_factor).to(float)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim).to(float)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim).to(float)\n",
    "    def forward(self,x):\n",
    "        x = x.to(float)\n",
    "        y = self.mha(x,None,None,mask=None).to(float)\n",
    "        o = self.ln1(y+x)\n",
    "        o2 = self.mlp(o)\n",
    "        return self.ln2(o2+o).to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3400,
   "id": "a0ed8e70-3225-4219-b905-f7db5fb3b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,emb_dim,num_heads,exp_factor=4,n_layers=6):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim \n",
    "        self.num_heads = num_heads\n",
    "        self.exp_factor = exp_factor\n",
    "        self.n_layers = n_layers\n",
    "        self.seq = nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            attn = Attention(emb_dim,num_heads,exp_factor).to(float)\n",
    "            self.seq.append(attn)\n",
    "    def forward(self,x):\n",
    "        x = x.to(float)\n",
    "        output = x\n",
    "        for i in self.seq:\n",
    "            output = i(output)\n",
    "        return output.to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3402,
   "id": "de59afc5-0722-43fc-9b60-9909aa7b9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder_block(nn.Module):\n",
    "    def __init__(self,emb_dim,num_heads,exp_factor):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttentionWithCrossAttention(emb_dim,num_heads).to(float)\n",
    "        self.mlp = MLP(emb_dim,exp_factor).to(float)\n",
    "        self.ln1 = nn.LayerNorm(emb_dim).to(float)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim).to(float)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim).to(float)\n",
    "    def forward(self,x,input,mask=None):\n",
    "        x = x.to(float)\n",
    "        input = input.to(float)\n",
    "        output = self.mha(x,mask=mask)\n",
    "        l1 = self.ln1(x+output)\n",
    "        cha = self.mha(l1,input,input)\n",
    "        l2 = self.ln2(cha+l1)\n",
    "        o1 = self.mlp(l2)\n",
    "        return self.ln3(o1+l2).to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3404,
   "id": "7a446c29-20bc-43aa-877d-05e5e2de9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,emb_dim,num_heads,exp_factor,n_layers=6):\n",
    "        super().__init__()\n",
    "        self.seq  =nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            d  = decoder_block(emb_dim,num_heads,exp_factor=4).to(float)\n",
    "            self.seq.append(d)\n",
    "    def forward(self,x,y,mask=None):\n",
    "        x = x.to(float)\n",
    "        y = y.to(float)\n",
    "        output  = x\n",
    "        for i in self.seq:\n",
    "            output = i(output,y,mask = mask)\n",
    "        return output.to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3418,
   "id": "2cf32f3b-076e-4c5c-a248-493e4abc6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,emb_dim,num_heads,vocab_size,exp_factor=4,n_layers=6):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(emb_dim,num_heads,exp_factor=4,n_layers=6)\n",
    "        self.dec = Decoder(emb_dim,num_heads,exp_factor=4,n_layers=6)\n",
    "# Shared weight matrix\n",
    "        self.num_heads=num_heads\n",
    "        self.emb_weight = nn.Parameter(\n",
    "            torch.randn(vocab_size, emb_dim, dtype=float)*math.sqrt(emb_dim)\n",
    "        )\n",
    "\n",
    "        # Embedding layer (ensure weight dtype matches)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim).to(float)\n",
    "        self.embedding.weight = self.emb_weight\n",
    "\n",
    "        # Linear layer (tied weights, ensure dtype matches)\n",
    "        self.linear = nn.Linear(emb_dim, vocab_size, bias=False).to(float)\n",
    "        self.linear.weight = self.emb_weight\n",
    "        # self.linear = nn.Linear(emb_dim,vocab_size).to(float)\n",
    "        # self.embedding = nn.Embedding(vocab_size,emb_dim).to(float)\n",
    "    def forward(self, x, output, mask=None):\n",
    "        # print(f\"Input x shape: {x.shape}\")\n",
    "        # print(f\"Input output shape: {output.shape}\")\n",
    "    \n",
    "        x = self.embedding(x).to(float)\n",
    "        output = self.embedding(output).to(float)\n",
    "        \n",
    "        # print(f\"Embedded x shape: {x.shape}\")\n",
    "        # print(f\"Embedded output shape: {output.shape}\")\n",
    "    \n",
    "        y = self.enc(x)\n",
    "        # print(f\"Encoder output y shape: {y.shape}\")\n",
    "    \n",
    "        o1 = self.dec(output, y, mask)\n",
    "        # print(f\"Decoder output o1 shape: {o1.shape}\")\n",
    "    \n",
    "        o2 = self.linear(o1).to(float)\n",
    "        # print(f\"Linear output o2 shape: {o2.shape}\")\n",
    "    \n",
    "        output = torch.softmax(o2, dim=-1)\n",
    "        # print(f\"Softmax output shape: {output.shape}\")\n",
    "    \n",
    "        return output.to(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3609,
   "id": "d5f5f9af-d446-4ff8-982e-61fbd67e3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(4,4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3611,
   "id": "63e9fdc6-11d2-46d8-bfba-5f4175975a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3613,
   "id": "894cfbd0-5081-4d27-8706-d298debc171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='exp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3654,
   "id": "9d88e2f3-2801-4fad-87f0-9a75978074da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Average Loss: 1.2331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:17, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Average Loss: 1.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Average Loss: 1.2319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Average Loss: 1.2194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:17, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Average Loss: 1.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Average Loss: 1.1746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:17, 17.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Average Loss: 1.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:18, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Average Loss: 1.1748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Average Loss: 1.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Average Loss: 1.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:17, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Average Loss: 1.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Average Loss: 1.1742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Average Loss: 1.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Average Loss: 1.1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Average Loss: 1.1747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Average Loss: 1.1735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25 - Average Loss: 1.1749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25 - Average Loss: 1.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25 - Average Loss: 1.1747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25 - Average Loss: 1.1743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25 - Average Loss: 1.1735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25 - Average Loss: 1.1749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25 - Average Loss: 1.1736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25 - Average Loss: 1.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [00:16, 19.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25 - Average Loss: 1.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "epochs=25\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = [] \n",
    "    for j,i in tqdm(enumerate(dataloader)):\n",
    "        inp = [j[0] for j in i] \n",
    "        out = [j[1] for j in i]\n",
    "        X = torch.tensor(np.array(inp))\n",
    "        Y = torch.tensor(np.array(out))\n",
    "        bat,s = Y.shape\n",
    "        a = torch.tensor([4])\n",
    "        a = a.expand(bat,1)\n",
    "        y_dec = torch.cat([a,Y],dim=-1)\n",
    "        X = torch.cat([a,X],dim=-1)\n",
    "        mask = create_decoder_mask(y_dec)\n",
    "        num_heads = model.num_heads  # Assuming your Transformer model has this attribute\n",
    "        mask = mask.unsqueeze(1).expand(-1, num_heads, -1, -1)\n",
    "        b = torch.tensor([5])\n",
    "        b = b.expand(bat,1)\n",
    "        y_true = torch.cat([Y,b],dim=-1)\n",
    "        y_out = model(X,y_dec,mask)\n",
    "        # print(y_out.shape)\n",
    "        y_predicted = torch.argmax(y_out,dim=-1)\n",
    "        # print(y_true.shape,y_predicted.shape)\n",
    "        y_out2 = y_out.view(-1,6)\n",
    "        y_true2 = y_true.view(-1)\n",
    "        # print(y_out2.shape,y_true2.shape)\n",
    "        loss = criterion(y_out2,y_true2)\n",
    "        # print(loss.item())\n",
    "        # writer.add_scalar('loss',loss.item(),global_step=i)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(loss.item())\n",
    "        epoch_losses.append(loss.item())\n",
    "        # print(epoch,len(dataloader),i)\n",
    "        writer.add_scalar('batch_loss', loss.item(), global_step=epoch * int(len(dataloader)) + j)\n",
    "    \n",
    "    # Calculate and log average epoch loss\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    writer.add_scalar('epoch_loss', avg_epoch_loss, global_step=epoch)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3637,
   "id": "3f3424e9-b6db-4129-b1ea-292b04865c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3656,
   "id": "bd077ab0-539c-4269-b79f-6ece794ddf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 49.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3113 - BLEU Score: 0.00\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "True Label: 0 0 1 1 1<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 0 0 1 1 <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# After training loop\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for j, i in tqdm(enumerate(val_dataloader)):\n",
    "        inp = [j[0] for j in i]\n",
    "        out = [j[1] for j in i]\n",
    "        X = torch.tensor(np.array(inp))\n",
    "        Y = torch.tensor(np.array(out))\n",
    "        bat, s = Y.shape\n",
    "        \n",
    "        # Add start token (e.g., 4) to the input and output sequences\n",
    "        a = torch.tensor([4])\n",
    "        a = a.expand(bat, 1)\n",
    "        y_dec = torch.cat([a, Y], dim=-1)\n",
    "        X = torch.cat([a, X], dim=-1)\n",
    "        \n",
    "        # Create decoder mask\n",
    "        mask = create_decoder_mask(y_dec)\n",
    "        num_heads = model.num_heads\n",
    "        mask = mask.unsqueeze(1).expand(-1, num_heads, -1, -1)\n",
    "        \n",
    "        # Add end token (e.g., 5) to the true labels\n",
    "        b = torch.tensor([5])\n",
    "        b = b.expand(bat, 1)\n",
    "        y_true = torch.cat([Y, b], dim=-1)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_out = model(X, y_dec, mask)\n",
    "        y_predicted = torch.argmax(y_out, dim=-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        y_out2 = y_out.view(-1, 6)  # Reshape for loss calculation\n",
    "        y_true2 = y_true.view(-1)   # Flatten true labels\n",
    "        loss = criterion(y_out2, y_true2)\n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "        # Decode predictions and labels\n",
    "        decoded_strings = [\"\".join(reverse_vocab[i.item()] for i in row) for row in y_predicted]\n",
    "        decoded_true = [\"\".join(reverse_vocab[i.item()] for i in row) for row in y_true]\n",
    "    \n",
    "    print(f\"Test Loss: {avg_test_loss:.4f} - BLEU Score: {avg_bleu_score:.2f}\")\n",
    "    \n",
    "    # Print example prediction\n",
    "    if len(decoded_strings) > 0:\n",
    "        print(\"\\nExample Prediction:\")\n",
    "        print(f\"Input: {inp[0]}\")\n",
    "        print(f\"True Label: {decoded_true[0]}\")\n",
    "        print(f\"Predicted Label: {decoded_strings[0]}\")\n",
    "\n",
    "# Reset model to training mode\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3658,
   "id": "d0937a60-5647-4902-ae46-14e2ecb40676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3660,
   "id": "f8719910-a228-45fb-81f7-a29a5be47250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Prediction:\n",
      "Input: tensor([1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "True Label: 0 0 1 1 1<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 0 0 1 1 <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 0, 2, 0, 3, 3, 3, 3])\n",
      "True Label: 0 0 0 0 1 1 1 1<pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 1 1 0 0 1 1 1<pad><pad><pad><pad><pad>\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([1, 2, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 3, 3, 3, 3, 3])\n",
      "True Label: 0 0 0 1 1 1 1<pad><pad><pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 0 0 1 1 1<pad>1<pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([0, 2, 0, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "True Label: 0 0 1<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 0 0<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([1, 2, 0, 2, 1, 2, 1, 2, 0, 2, 1, 2, 1, 3, 3, 3, 3, 3, 3])\n",
      "True Label: 0 0 1 1 1 1 1<pad><pad><pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 0 0 1 1 1 1<pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0])\n",
      "True Label: 0 0 0 0 0 1 1 1 1 1<eos>\n",
      "Predicted Label: 0 0 0 0 0 1 1 1 1 1 \n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([1, 2, 1, 2, 0, 2, 1, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
      "True Label: 0 1 1 1 1<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><eos>\n",
      "Predicted Label: 0 0 0 1 1 <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "Example Prediction:\n",
      "Input: tensor([0, 2, 1, 2, 0, 2, 1, 2, 1, 2, 0, 2, 1, 2, 1, 2, 1, 2, 0])\n",
      "True Label: 0 0 0 0 1 1 1 1 1 1<eos>\n",
      "Predicted Label: 0 0 0 1 1 1 1 1 1 1 \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(decoded_strings)):\n",
    "    print(\"\\nExample Prediction:\")\n",
    "    print(f\"Input: {inp[i]}\")\n",
    "    print(f\"True Label: {decoded_true[i]}\")\n",
    "    print(f\"Predicted Label: {decoded_strings[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3641,
   "id": "fdc339de-1dff-4d51-9077-5dea4bcb943e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 20, 6]), torch.Size([8, 20]))"
      ]
     },
     "execution_count": 3641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_out.shape , y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3430,
   "id": "8cbbf8e1-7e99-491b-89c0-8aea84c5e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'0': 0, '1': 1, ' ': 2, '<pad>': 3,'<sos>':4,'<eos>':5}\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3434,
   "id": "b0ae8645-fd44-4f90-91d2-f0283714c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_strings = [\"\".join(reverse_vocab[i.item()] for i in row) for row in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3438,
   "id": "391a48a9-1af5-492a-abfa-2a4f39552663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad><pad>0<pad><pad><pad>0<pad>0<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 3438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3410,
   "id": "2129677c-920a-47c7-9e1f-4788ec6bc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class NumberSortingDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, max_length=10):\n",
    "        self.num_samples = num_samples\n",
    "        self.max_length = max_length\n",
    "        self.vocab = {'0': 0, '1': 1, ' ': 2, '<pad>': 3,'<sos>':4,'<eos>':5}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random numbers\n",
    "        length = random.randint(3, self.max_length)\n",
    "        numbers = [str(random.randint(0, 1)) for _ in range(length)]\n",
    "        input_seq = ' '.join(numbers)\n",
    "        target_seq = ' '.join(sorted(numbers))  # Sorted sequence\n",
    "        \n",
    "        # Convert to index list\n",
    "        input_tensor = [self.vocab[c] for c in input_seq]\n",
    "        target_tensor = [self.vocab[c] for c in target_seq]\n",
    "\n",
    "        # Compute padding length\n",
    "        pad_length = self.max_length * 2 - 1\n",
    "        input_tensor += [self.vocab['<pad>']] * (pad_length - len(input_tensor))\n",
    "        target_tensor += [self.vocab['<pad>']] * (pad_length - len(target_tensor))\n",
    "\n",
    "        # Convert to PyTorch tensors after padding\n",
    "        return torch.tensor(input_tensor, dtype=torch.long), torch.tensor(target_tensor, dtype=torch.long)\n",
    "\n",
    "# Initialize\n",
    "dataset = NumberSortingDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=lambda x: x)\n",
    "val_dataset = NumberSortingDataset(num_samples=1000)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3412,
   "id": "c5faaa5b-d7a1-4d39-9766-2d9923df5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3390,
   "id": "f62d8bb5-8bc6-4bc1-a797-ba32b5361214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 3390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3392,
   "id": "ee599da8-db27-462c-aec4-bf2fc20445ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [ 1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3332,
   "id": "a9122f51-b4eb-47e3-b3c3-d707002195da",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [i[0] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3334,
   "id": "178e2287-a070-4e69-b115-f2d0adfb8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [i[1] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3336,
   "id": "e41c18de-636c-41e0-a7eb-ba546662870d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3336], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inp[\u001b[38;5;241m7\u001b[39m] , out[\u001b[38;5;241m7\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "inp[7] , out[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3338,
   "id": "702d1f5d-3372-4f90-9e95-838b87199c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = torch.tensor(np.array(inp)).unsqueeze(dim=-1)\n",
    "Y = torch.tensor(np.array(out)).unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3340,
   "id": "fd1b5e63-dcac-4200-ae90-4b5b97e996b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3342,
   "id": "539f566d-184e-4c25-8a52-1755c8045715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3344,
   "id": "da9f4481-57dc-4c12-8d7e-995c7de5136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[0], Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3346,
   "id": "8aec0007-1044-467a-8bd1-a221b707c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(20,1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0a04e-e0a2-4f66-9849-564c5e3f1a45",
   "metadata": {},
   "source": [
    "model(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3349,
   "id": "04512c75-0ceb-41c8-8c0c-4cac7ec91f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x shape: torch.Size([2, 4, 1])\n",
      "Input output shape: torch.Size([2, 4, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3349], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X,Y)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3322], line 25\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, output, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput x shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(output)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedded x shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "output = model(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3350,
   "id": "9d35bcf2-f4e2-44d6-a301-9fb1d5e65e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 3350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3353,
   "id": "8970cb29-b5d5-4918-a2ab-358c02717ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.argmax(output,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3355,
   "id": "1c192985-6767-42f6-894f-84c25a62b8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 3355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3357,
   "id": "63c3c5ad-2267-43bc-ae07-7a6cce80d945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x304269bb0>"
      ]
     },
     "execution_count": 3357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3359,
   "id": "e8d49672-bf48-493e-ad61-0e9df988deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_mask(y_dec, pad_token=3):\n",
    "    batch_size, seq_len = y_dec.shape\n",
    "\n",
    "    # Padding mask (True for valid tokens, False for pad tokens)\n",
    "    padding_mask = y_dec != pad_token  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Causal mask (Look-ahead mask)\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool))  # (seq_len, seq_len)\n",
    "\n",
    "    # Combine both: expand padding mask to match causal mask dimensions\n",
    "    final_mask = padding_mask.unsqueeze(1) & causal_mask  # Shape: (batch_size, seq_len, seq_len)\n",
    "    return ~final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3361,
   "id": "8c09c28a-bba8-4647-a427-56b865d65e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/313 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3361], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m b,s \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     11\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m y_dec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([a,Y],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_dec\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# y_pred = model(X,Y)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in tqdm(dataloader):\n",
    "        inp = [j[0] for j in i] \n",
    "        out = [j[1] for j in i]\n",
    "        X = torch.tensor(np.array(inp))\n",
    "        Y = torch.tensor(np.array(out))\n",
    "        b,s = Y.shape\n",
    "        a = torch.tensor(4)\n",
    "        y_dec = torch.cat([a,Y],dim=-1)\n",
    "        print(y_dec.shape)\n",
    "        \n",
    "        # y_pred = model(X,Y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3363,
   "id": "adb1c2f3-4b26-4a3c-b732-7ba351af177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fcffd-4db7-4634-a6db-f58bd0f8f031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3366,
   "id": "1bd1d089-9cdd-4e20-bd45-928f07f60c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(7,1,6)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for i in tqdm(dataloader):\n",
    "    inp = [j[0] for j in i] \n",
    "    out = [j[1] for j in i]\n",
    "    X = torch.tensor(np.array(inp))\n",
    "    Y = torch.tensor(np.array(out))\n",
    "    bat,s = Y.shape\n",
    "    a = torch.tensor([4])\n",
    "    a = a.expand(bat,1)\n",
    "    y_dec = torch.cat([a,Y],dim=-1)\n",
    "    X = torch.cat([a,X],dim=-1)\n",
    "    mask = create_decoder_mask(y_dec)\n",
    "    num_heads = model.num_heads  # Assuming your Transformer model has this attribute\n",
    "    mask = mask.unsqueeze(1).expand(-1, num_heads, -1, -1)\n",
    "    b = torch.tensor([5])\n",
    "    b = b.expand(bat,1)\n",
    "    y_true = torch.cat([Y,b],dim=-1)\n",
    "    print(mask.shape)\n",
    "    # y_out = model(X,y_dec,mask)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24193a-dcaa-475b-a612-25abdd76aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(y_out,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3205,
   "id": "22c80dd7-691e-4dc1-8d0d-0223c92e9337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 20]), torch.Size([32, 20]))"
      ]
     },
     "execution_count": 3205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape , y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3207,
   "id": "1aa4f1ad-b802-4d87-b379-a11c9be0a9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 0, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1]),\n",
       " tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 3]))"
      ]
     },
     "execution_count": 3207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dec[0] , y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3080,
   "id": "886ea55e-23bc-4522-a2dd-5f47db8dda9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 3080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3027,
   "id": "d1f133fe-2f1c-44a6-b3f2-0062c002f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_decoder_mask(y_dec, pad_token=3):\n",
    "    \"\"\"\n",
    "    Creates a mask for the decoder input where padding tokens are marked as False.\n",
    "\n",
    "    Args:\n",
    "        y_dec (torch.Tensor): Tensor of shape (batch_size, seq_len) containing tokenized sequences.\n",
    "        pad_token (int): The token representing padding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Boolean mask of shape (batch_size, seq_len) where True means valid token, False means padding.\n",
    "    \"\"\"\n",
    "    return y_dec != pad_token  # Mask where True means the token is valid\n",
    "\n",
    "# Example usage\n",
    "y_dec = torch.tensor([\n",
    "    [4, 0, 2, 0, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
    "    [4, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n",
    "])  # Example input\n",
    "\n",
    "mask = create_decoder_mask(y_dec)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2500,
   "id": "27b36e03-9b97-4991-bebc-487137288098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
      "         [False, False, False, False, False, False, False,  True,  True,  True],\n",
      "         [False, False, False, False, False, False, False, False,  True,  True],\n",
      "         [False, False, False, False, False, False, False, False, False,  True],\n",
      "         [False, False, False, False, False, False, False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_decoder_mask(y_dec, pad_token=3):\n",
    "    batch_size, seq_len = y_dec.shape\n",
    "\n",
    "    # Padding mask (True for valid tokens, False for pad tokens)\n",
    "    padding_mask = y_dec != pad_token  # Shape: (batch_size, seq_len)\n",
    "\n",
    "    # Causal mask (Look-ahead mask)\n",
    "    causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool))  # (seq_len, seq_len)\n",
    "\n",
    "    # Combine both: expand padding mask to match causal mask dimensions\n",
    "    final_mask = padding_mask.unsqueeze(1) & causal_mask  # Shape: (batch_size, seq_len, seq_len)\n",
    "    return ~final_mask\n",
    "\n",
    "# Example usage\n",
    "y_dec = torch.tensor([\n",
    "    [4, 0, 2, 0, 2, 1, 3, 3, 3, 3],\n",
    "    [4, 0, 2, 0, 2, 0, 2, 1, 2, 1]\n",
    "])  # Example input\n",
    "\n",
    "mask = create_decoder_mask(y_dec)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d18cb-1122-40a8-b842-55bc5d4d386b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "id": "247e7fa1-f950-4339-bce7-12654c961ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 1487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "id": "c34e418b-c4d3-4aaf-b67e-388414827cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1376], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_fill(y_dec,mask,\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.masked_fill(y_dec,mask,-torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c45c1-085f-4857-992a-3771549ed585",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.masked_fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427f0a2-3419-47b5-89a6-fc95056df032",
   "metadata": {},
   "outputs": [],
   "source": [
    "<sos>daksh is GooD boy<eos><pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3850,
   "id": "dec06898-79bc-4945-bace-2d790d9e8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(emb):\n",
    "    b,s,d = emb.shape\n",
    "    for i in range(1,s+1):\n",
    "        for j in range(d):\n",
    "            if(j%2==0):\n",
    "                # print(emb[:,i2,j])\n",
    "                # print(torch.sin((i2)/(10000**(2j/d))))\n",
    "                emb[:,i-1,j]+=math.sin(i/(10000**(2*j/d)))\n",
    "            else:\n",
    "                emb[:,i-1,j]+=math.cos(i/(10000**(2*j/d)))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3852,
   "id": "58e34ece-5f3b-4835-b6ea-67411d44f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp  = torch.randn(2,4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3854,
   "id": "a80a41cd-702c-4d4d-b7fa-0749ef0a56bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 3854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_encoding(temp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3842,
   "id": "af856399-4409-4b56-9732-e09176c044e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 3842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3742,
   "id": "5371acae-49fc-48c0-9e77-69f560e9d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[:,0,0]+=math.sin(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3744,
   "id": "be5e4f07-f585-4ab2-869f-cf9f6db988ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2385, 1.4396])"
      ]
     },
     "execution_count": 3744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3822,
   "id": "8a1ab8ae-f896-4440-8488-7b2bb3a7b12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b,s,d = temp.shape\n",
    "i2 = 0 \n",
    "j=0\n",
    "i2/(10000**(2*j/d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3824,
   "id": "0166b750-5ad9-4221-8a14-3a99a979bdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3820,
   "id": "224bd37e-da5e-489a-b54d-0428e086d3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2/1000**(2*j/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3860,
   "id": "58b625dc-ab01-427f-8584-0e5585874db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,df):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "    def len(self):\n",
    "        return len(self.df)\n",
    "    def get(self,idx):\n",
    "        return self.df.iloc[idx][0],self.df.iloc[idx][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4274e-753c-4ca4-aae5-a1d35c720bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc09942-5f74-4f02-9084-995444dd0eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
